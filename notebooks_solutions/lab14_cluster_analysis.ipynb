{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce the decision tree visualization you should install the graphviz package into your system: \n",
    "https://stackoverflow.com/questions/35064304/runtimeerror-make-sure-the-graphviz-executables-are-on-your-systems-path-aft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one of these in case you have problems with graphviz\n",
    "\n",
    "# All users: try this first\n",
    "# ! conda install graphviz\n",
    "\n",
    "# If that doesn't work:\n",
    "# Ubuntu/Debian users only\n",
    "# ! sudo apt-get update && sudo apt-get install graphviz\n",
    "\n",
    "# Mac users only (assuming you have homebrew installed)\n",
    "# ! brew install graphviz\n",
    "\n",
    "# Windows users, check the stack overflow link. Sorry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "import graphviz\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join('..', 'data', 'tugas_preprocessed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting feature names into groups\n",
    "non_metric_features = df.columns[df.columns.str.startswith('x')]\n",
    "pc_features = df.columns[df.columns.str.startswith('PC')]\n",
    "metric_features = df.columns[~df.columns.str.startswith('x') & ~df.columns.str.startswith('PC')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we proceed\n",
    "\n",
    "- Consider applying the outlier filtering method discussed last class.\n",
    "    - We manually filtered the dataset's outliers based on a univariate analysis\n",
    "- Consider dropping/transforming the variable \"rcn\". Why?\n",
    "    - Very little correlation with any other variables\n",
    "    - Remember the Component planes: the SOM's units were indistinguishable on this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the hyperparameters found in the previous class\n",
    "dbscan = DBSCAN(eps=1.9, min_samples=20, n_jobs=4)\n",
    "dbscan_labels = dbscan.fit_predict(df[metric_features])\n",
    "Counter(dbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the newly detected outliers (they will be classified later based on the final clusters)\n",
    "df_out = df[dbscan_labels==-1].copy()\n",
    "\n",
    "# New df without outliers and 'rcn'\n",
    "df = df[dbscan_labels!=-1]\\\n",
    "    .drop(columns='rcn')\\\n",
    "    .copy()\n",
    "\n",
    "# Update metric features list\n",
    "metric_features = metric_features.drop('rcn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering by Perspectives\n",
    "- Demographic/Behavioral Perspective:\n",
    "- Product Perspective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split variables into perspectives (example, requires critical thinking and domain knowledge)\n",
    "demographic_features = [\n",
    "    'income',\n",
    "    'frq',\n",
    "    'per_net_purchase',\n",
    "    'spent_online'\n",
    "]\n",
    "\n",
    "preference_features = [\n",
    "    'clothes', \n",
    "    'kitchen', \n",
    "    'small_appliances',\n",
    "    'toys', \n",
    "    'house_keeping', \n",
    "]\n",
    "\n",
    "df_dem = df[demographic_features].copy()\n",
    "df_prf = df[preference_features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on K-means and Hierarchical clustering\n",
    "Based on (1) our previous tests and (2) the context of this problem, the optimal number of clusters is expected to be between 3 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df):\n",
    "    \"\"\"Computes the sum of squares for all variables given a dataset\n",
    "    \"\"\"\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable\n",
    "\n",
    "def r2(df, labels):\n",
    "    sst = get_ss(df)\n",
    "    ssw = np.sum(df.groupby(labels).apply(get_ss))\n",
    "    return 1 - ssw/sst\n",
    "    \n",
    "def get_r2_scores(df, clusterer, min_k=2, max_k=10):\n",
    "    \"\"\"\n",
    "    Loop over different values of k. To be used with sklearn clusterers.\n",
    "    \"\"\"\n",
    "    r2_clust = {}\n",
    "    for n in range(min_k, max_k):\n",
    "        clust = clone(clusterer).set_params(n_clusters=n)\n",
    "        labels = clust.fit_predict(df)\n",
    "        r2_clust[n] = r2(df, labels)\n",
    "    return r2_clust\n",
    "\n",
    "\n",
    "# Set up the clusterers\n",
    "kmeans = KMeans(\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hierarchical = AgglomerativeClustering(\n",
    "    metric='euclidean'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal clusterer on demographic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on demographic variables\n",
    "r2_scores = {}\n",
    "r2_scores['kmeans'] = get_r2_scores(df_dem, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores[linkage] = get_r2_scores(\n",
    "        df_dem, hierarchical.set_params(linkage=linkage)\n",
    "    )\n",
    "\n",
    "pd.DataFrame(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the R² scores for each cluster solution on demographic variables\n",
    "pd.DataFrame(r2_scores).plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Demographic Variables:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R² metric\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat the process for product variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on product variables\n",
    "r2_scores = {}\n",
    "r2_scores['kmeans'] = get_r2_scores(df_prf, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores[linkage] = get_r2_scores(\n",
    "        df_prf, hierarchical.set_params(linkage=linkage)\n",
    "    )\n",
    "\n",
    "# Visualizing the R² scores for each cluster solution on product variables\n",
    "pd.DataFrame(r2_scores).plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Product Variables:\\nR2 plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R2 metric\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the Perspectives\n",
    "- How can we merge different cluster solutions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the right clustering (algorithm and number of clusters) for each perspective\n",
    "kmeans_prod = KMeans(\n",
    "    n_clusters=3,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "prod_labels = kmeans_prod.fit_predict(df_prf)\n",
    "\n",
    "kmeans_behav = KMeans(\n",
    "    n_clusters=4,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "behavior_labels = kmeans_behav.fit_predict(df_dem)\n",
    "\n",
    "df['product_labels'] = prod_labels\n",
    "df['behavior_labels'] = behavior_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count label frequencies (contigency table)\n",
    "\n",
    "pd.crosstab(df['behavior_labels'],\n",
    "            df['product_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual merging: Merge lowest frequency clusters into closest clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroids of clusters\n",
    "df_centroids = df.groupby(['behavior_labels', 'product_labels'])\\\n",
    "    [metric_features].mean()\n",
    "\n",
    "\n",
    "# Clusters with low frequency to be merged:\n",
    "# (behavior_labels, product_labels)\n",
    "to_merge = [(3,2), (3,0), (2,2), (0,2)]\n",
    "\n",
    "\n",
    "# Computing the euclidean distance matrix between the centroids\n",
    "euclidean = pairwise_distances(df_centroids)\n",
    "df_dists = pd.DataFrame(\n",
    "    euclidean, columns=df_centroids.index, index=df_centroids.index\n",
    ")\n",
    "\n",
    "# Merging each low frequency clustering (source) to the closest cluster (target)\n",
    "source_target = {}\n",
    "for clus in to_merge:\n",
    "    if clus not in source_target.values():\n",
    "        source_target[clus] = df_dists.loc[clus].sort_values().index[1]\n",
    "\n",
    "source_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "\n",
    "# Changing the behavior_labels and product_labels based on source_target\n",
    "for source, target in source_target.items():\n",
    "    mask = (df_['behavior_labels']==source[0]) & (df_['product_labels']==source[1])\n",
    "    df_.loc[mask, 'behavior_labels'] = target[0]\n",
    "    df_.loc[mask, 'product_labels'] = target[1]\n",
    "\n",
    "# New contigency table\n",
    "\n",
    "pd.crosstab(df_['behavior_labels'],\n",
    "            df_['product_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging using Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroids of the concatenated cluster labels\n",
    "df_centroids = df.groupby(['behavior_labels', 'product_labels'])\\\n",
    "    [metric_features].mean()\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hierarchical clustering to merge the concatenated cluster centroids\n",
    "linkage = 'ward'\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage=linkage, \n",
    "    metric='euclidean', \n",
    "    distance_threshold=0, \n",
    "    n_clusters=None\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = 2.3\n",
    "dendrogram(linkage_matrix, \n",
    "           truncate_mode='level', \n",
    "           labels=df_centroids.index, p=5, \n",
    "           color_threshold=y_threshold, \n",
    "           above_threshold_color='k')\n",
    "\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'Euclidean Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-running the Hierarchical clustering based on the correct number of clusters\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    metric='euclidean', \n",
    "    n_clusters=7\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)\n",
    "df_centroids['hclust_labels'] = hclust_labels\n",
    "\n",
    "df_centroids  # centroid's cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper between concatenated clusters and hierarchical clusters\n",
    "cluster_mapper = df_centroids['hclust_labels'].to_dict()\n",
    "\n",
    "df_ = df.copy()\n",
    "\n",
    "# Mapping the hierarchical clusters on the centroids to the observations\n",
    "df_['merged_labels'] = df_.apply(\n",
    "    lambda row: cluster_mapper[\n",
    "        (row['behavior_labels'], row['product_labels'])\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "# Merged cluster centroids\n",
    "df_.groupby('merged_labels').mean()[metric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge cluster contigency table\n",
    "# Getting size of each final cluster\n",
    "df_counts = df_.groupby('merged_labels')\\\n",
    "    .size()\\\n",
    "    .to_frame()\n",
    "\n",
    "# Getting the product and behavior labels\n",
    "df_counts = df_counts\\\n",
    "    .rename({v:k for k, v in cluster_mapper.items()})\\\n",
    "    .reset_index()\n",
    "\n",
    "df_counts['behavior_labels'] = df_counts['merged_labels'].apply(lambda x: x[0])\n",
    "df_counts['product_labels'] = df_counts['merged_labels'].apply(lambda x: x[1])\n",
    "\n",
    "df_counts.pivot(values=0, index='behavior_labels', columns='product_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting df to have the final product, behavior and merged clusters\n",
    "df = df_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, compar_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    if compar_titles == None:\n",
    "        compar_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2, figsize=figsize, squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, label, color=sns.color_palette(), ax=ax[0])\n",
    "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1])\n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].legend(handles, cluster_labels) # Adaptable to number of clusters\n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=-20)\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profilling each cluster (product, behavior, merged)\n",
    "cluster_profiles(\n",
    "    df = df[metric_features.to_list() + ['product_labels', 'behavior_labels', 'merged_labels']], \n",
    "    label_columns = ['product_labels', 'behavior_labels', 'merged_labels'], \n",
    "    figsize = (28, 13), \n",
    "    compar_titles = [\"Product clustering\", \"Behavior clustering\", \"Merged clusters\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: profiling with unused / categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_metric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_educ = df[['merged_labels',\n",
    "              'x_education_2nd Cycle', \n",
    "              'x_education_Graduation', \n",
    "              'x_education_Master',\n",
    "               'x_education_PhD']].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_educ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "df_educ.plot(kind='bar', stacked=False, ax=ax)\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "df_educ_pct = df_educ.apply(lambda col: col/df_educ.sum(axis=1))\n",
    "df_educ_pct.plot(kind='bar', stacked=False, ax=ax)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_educ_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster visualization using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is step can be quite time consuming\n",
    "two_dim = TSNE(random_state=42).fit_transform(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=df['merged_labels'], colormap='tab10', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess feature importance and reclassify outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the R²\n",
    "What proportion of each variables total SS is explained between clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_variables(df):\n",
    "    \"\"\"Get the SS for each variable\n",
    "    \"\"\"\n",
    "    ss_vars = df.var() * (df.count() - 1)\n",
    "    return ss_vars\n",
    "\n",
    "def r2_variables(df, labels):\n",
    "    \"\"\"Get the R² for each variable\n",
    "    \"\"\"\n",
    "    sst_vars = get_ss_variables(df)\n",
    "    ssw_vars = np.sum(df.groupby(labels).apply(get_ss_variables))\n",
    "    return 1 - ssw_vars/sst_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are essentially decomposing the R² into the R² for each variable\n",
    "r2_variables(df[metric_features.to_list() + ['merged_labels']], 'merged_labels').drop('merged_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Decision Tree\n",
    "We get the normalized total reduction of the criterion (gini or entropy) brought by that feature (also known as Gini importance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X = df.drop(columns=['product_labels','behavior_labels','merged_labels'])\n",
    "y = df.merged_labels\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fitting the decision tree\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"It is estimated that in average, we are able to predict {0:.2f}% of the customers correctly\".format(dt.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing feature importance\n",
    "pd.Series(dt.feature_importances_, index=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the cluster labels of the outliers\n",
    "df_out['merged_labels'] = dt.predict(df_out.drop(columns=['rcn']))\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the decision tree\n",
    "dot_data = export_graphviz(dt, out_file=None, \n",
    "                           feature_names=X.columns.to_list(),\n",
    "                           filled=True,\n",
    "                           rounded=True,\n",
    "                           special_characters=True)\n",
    "g = graphviz.Source(dot_data)\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "\n",
    "# png = g.pipe(format='png')\n",
    "# with open('dt.png','wb') as f:\n",
    "#     f.write(png)\n",
    "\n",
    "# Image(png_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
