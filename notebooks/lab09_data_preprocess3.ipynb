{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: library imports are ALWAYS at the top of the script, no exceptions!\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "The data we will be using through the pratical classes comes from a small relational database whose schema can be seen below:\n",
    "![alt text](../figures/schema.png \"Relation database schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to database\n",
    "my_path = os.path.join(\"..\", \"data\", \"datamining.db\")\n",
    "\n",
    "# connect to the database\n",
    "conn = sqlite3.connect(my_path)\n",
    "\n",
    "# the query\n",
    "query = \"\"\"\n",
    "select\n",
    "    age, \n",
    "    income, \n",
    "    frq, \n",
    "    rcn, \n",
    "    mnt, \n",
    "    clothes, \n",
    "    kitchen, \n",
    "    small_appliances, \n",
    "    toys, \n",
    "    house_keeping,\n",
    "    dependents, \n",
    "    per_net_purchase,\n",
    "    g.gender, \n",
    "    e.education, \n",
    "    m.status, \n",
    "    r.description\n",
    "from customers as c\n",
    "    join genders as g on g.id = c.gender_id\n",
    "    join education_levels as e on e.id = c.education_id\n",
    "    join marital_status as m on m.id = c.marital_status_id\n",
    "    join recommendations as r on r.id = c.recommendation_id\n",
    "order by c.id;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of your original dataset\n",
    "\n",
    "why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "- *id* - The unique identifier of the customer\n",
    "- *age* - The year of birht of the customer\n",
    "- *income* - The income of the customer\n",
    "- *frq* - Frequency: number of purchases made by the customer\n",
    "- *rcn* - Recency: number of days since last customer purchase\n",
    "- *mnt* - Monetary: amount of € spent by the customer in purchases\n",
    "- *clothes* - Number of clothes items purchased by the customer\n",
    "- *kitchen* - Number of kitchen items purchased by the customer\n",
    "- *small_appliances* - Number of small_appliances items purchased by the customer\n",
    "- *toys* - Number of toys items purchased by the customer\n",
    "- *house_keeping* - Number of house_keeping items purchased by the customer\n",
    "- *dependents* - Binary. Whether or not the customer has dependents\n",
    "- *per_net_purchase* - Percentage of purchases made online\n",
    "- *education* - Education level of the customer\n",
    "- *status* - Marital status of the customer\n",
    "- *gender* - Gender of the customer\n",
    "- *description* - Last customer's recommendation description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems:\n",
    "- Duplicates?\n",
    "- Data types?\n",
    "- Missing values?\n",
    "- Strange values?\n",
    "- Descriptive statistics?\n",
    "\n",
    "### Take a closer look and point out possible problems:\n",
    "\n",
    "(hint: a missing values in pandas is represented with a NaN value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"\" by nans\n",
    "df.replace(\"\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset data types again\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check descriptive statistics again\n",
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric and non-metric features. Why?\n",
    "non_metric_features = [\"education\", \"status\", \"gender\", \"dependents\", \"description\"]\n",
    "metric_features = df.columns.drop(non_metric_features).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values (Data imputation)\n",
    "\n",
    "How can we fill missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy to apply central tendency measures imputation\n",
    "df_central = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of missing values\n",
    "df_central.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_central[metric_features].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = df_central[non_metric_features].mode().loc[0]\n",
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_central.fillna(df_central[metric_features].median(), inplace=True)\n",
    "df_central.fillna(modes, inplace=True)\n",
    "df_central.isna().sum()  # checking how many NaNs we still have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new df copy to explore neighbordhood imputation\n",
    "df_neighbors = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing rows with NaNs\n",
    "nans_index = df_neighbors.isna().any(axis=1)\n",
    "df_neighbors[nans_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KNNImputer - only works for numerical varaibles\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "df_neighbors[metric_features] = imputer.fit_transform(df_neighbors[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See rows with NaNs imputed\n",
    "df_neighbors.loc[nans_index, metric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's keep the central imputation\n",
    "df = df_central.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An overview of our previous data exploration\n",
    "\n",
    "You can also explore this dataset using the exported `pandas-profiling` report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figures/exp_analysis/categorical_variables_frequecies.png)\n",
    "\n",
    "![](../figures/exp_analysis/numeric_variables_histograms.png)\n",
    "\n",
    "![](../figures/exp_analysis/numeric_variables_boxplots.png)\n",
    "\n",
    "![](../figures/exp_analysis/pairwise_relationship_of_numerical_variables.png)\n",
    "\n",
    "![](../figures/exp_analysis/correlation_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "\n",
    "Why do we need to remove outliers? Which methods can we use?\n",
    "\n",
    "\n",
    "Let's start by \"manually\" filtering the dataset's outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may vary from session to session, and is prone to varying interpretations.\n",
    "# A simple example is provided below:\n",
    "\n",
    "filters1 = (\n",
    "    (df['house_keeping']<=50)\n",
    "    &\n",
    "    (df['kitchen']<=40)\n",
    "    &\n",
    "    (df['toys']<=35)\n",
    "    &\n",
    "    (df['education']!='OldSchool')\n",
    ")\n",
    "\n",
    "df_1 = df[filters1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of data kept after removing outliers:', np.round(df_1.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal using only the IQR method\n",
    "\n",
    "Why should you use/not use this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q25 = df[metric_features].quantile(.25)\n",
    "q75 = df[metric_features].quantile(.75)\n",
    "iqr = (q75 - q25)\n",
    "\n",
    "upper_lim = q75 + 1.5 * iqr\n",
    "lower_lim = q25 - 1.5 * iqr\n",
    "\n",
    "filters2 = []\n",
    "for metric in metric_features:\n",
    "    llim = lower_lim[metric]\n",
    "    ulim = upper_lim[metric]\n",
    "    filters2.append(df[metric].between(llim, ulim, inclusive='both'))\n",
    "\n",
    "filters2 = pd.Series(np.all(filters2, 0))\n",
    "df_2 = df[filters2]\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_2.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about this percentage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining different outlier methods\n",
    "\n",
    "More robust/ consistent outlier detection method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df[(filters1 | filters2)]\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_3.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the manual filtering version\n",
    "df = df_1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "A reminder of our metadata:\n",
    "- *id* - The unique identifier of the customer\n",
    "- *age* - The year of birht of the customer\n",
    "- *income* - The income of the customer\n",
    "- *frq* - Frequency: number of purchases made by the customer\n",
    "- *rcn* - Recency: number of days since last customer purchase\n",
    "- *mnt* - Monetary: amount of € spent by the customer in purchases\n",
    "- *clothes* - Number of clothes items purchased by the customer\n",
    "- *kitchen* - Number of kitchen items purchased by the customer\n",
    "- *small_appliances* - Number of small_appliances items purchased by the customer\n",
    "- *toys* - Number of toys items purchased by the customer\n",
    "- *house_keeping* - Number of house_keeping items purchased by the customer\n",
    "- *dependents* - Binary. Whether or not the customer has dependents\n",
    "- *per_net_purchase* - Percentage of purchases made online\n",
    "- *education* - Education level of the customer\n",
    "- *status* - Marital status of the customer\n",
    "- *gender* - Gender of the customer\n",
    "- *description* - Last customer's recommendation description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['birth_year'] = df['age']\n",
    "df['age'] = datetime.now().year - df['birth_year']\n",
    "\n",
    "df['spent_online'] = (df['per_net_purchase'] / 100) * df['mnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we avoid having as many extreme values in 'rcn'?\n",
    "print((df['rcn']>100).value_counts())\n",
    "\n",
    "rcn_t = df['rcn'].copy()\n",
    "rcn_t.loc[rcn_t>100] = 100\n",
    "\n",
    "df['rcn'] = rcn_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection: Redundancy VS Relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundancy\n",
    "We already saw our original correlation matrix:\n",
    "![](../figures/exp_analysis/correlation_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables according to their correlations\n",
    "df.drop(columns=['birth_year', 'age', 'mnt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating metric_features\n",
    "metric_features.append(\"spent_online\")\n",
    "metric_features.remove(\"mnt\")\n",
    "metric_features.remove(\"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevancy\n",
    "Selecting variables based on the relevancy of each one to the task. Example: remove uncorrelated variables with the target, stepwise regression, use variables for product clustering, use variables for socio-demographic clustering, ...\n",
    "\n",
    "Variables that aren't correlated with any other variable are often also not relevant. In this case we will not focus on this a lot since we don't have a defined task yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MinMaxScaler to scale the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_feat = scaler.fit_transform(df_minmax[metric_features])\n",
    "scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the fit method is doing (notice the trailing underscore):\n",
    "print(\"Parameters fitted:\\n\", scaler.data_min_, \"\\n\", scaler.data_max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax[metric_features] = scaled_feat\n",
    "df_minmax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking max and min of minmaxed variables\n",
    "df_minmax[metric_features].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_feat = scaler.fit_transform(df_standard[metric_features])\n",
    "scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See what the fit method is doing (notice the trailing underscore):\n",
    "print(\"Parameters fitted:\\n\", scaler.mean_, \"\\n\", scaler.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard[metric_features] = scaled_feat\n",
    "df_standard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking mean and variance of standardized variables\n",
    "df_standard[metric_features].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: What if we had a training and test set? Should we fit a Scaler in both? What about other Sklearn objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_standard.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohc = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's remove status=Whatever\n",
    "df_ohc.loc[df_ohc['status'] == 'Whatever', 'status'] = df['status'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical features. Get feature names and create a DataFrame \n",
    "# with the one-hot encoded categorical features (pass feature names)\n",
    "ohc = OneHotEncoder(sparse_output=False, drop=\"first\")\n",
    "ohc_feat = ohc.fit_transform(df_ohc[non_metric_features])\n",
    "ohc_feat_names = ohc.get_feature_names_out()\n",
    "ohc_df = pd.DataFrame(ohc_feat, index=df_ohc.index, columns=ohc_feat_names)  # Why the index=df_ohc.index?\n",
    "ohc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain ohc variables\n",
    "df_ohc = pd.concat([df_ohc.drop(columns=non_metric_features), ohc_df], axis=1)\n",
    "df_ohc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ohc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename OHE columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename OHE columns from \"feature_val_a\" to \"x_feature_val_a\"\n",
    "## e.g. status_Widow to x_status_Widow\n",
    "## We do this to be able to distinguish the OHE columns more easily later\n",
    "\n",
    "## Assemble OHE columns and their new column names\n",
    "rename_ohe_cols = {}\n",
    "\n",
    "for i in non_metric_features:\n",
    "    for j in df.columns[df.columns.str.startswith(i)].to_list() :\n",
    "        rename_ohe_cols[j] = 'x_' + j\n",
    "\n",
    "df.rename(columns=rename_ohe_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [A more specific explanation of PCA](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)\n",
    "\n",
    "![](https://builtin.com/sites/www.builtin.com/files/inline-images/national/Principal%2520Component%2520Analysis%2520second%2520principal.gif)\n",
    "\n",
    "A more detailed explanation of PCA: 500 pages book\n",
    "\n",
    "Jolliffe, I. T. (2002). Principal component analysis. Springer New York.\n",
    " [[link]](http://cda.psych.uiuc.edu/statistical_learning_course/Jolliffe%20I.%20Principal%20Component%20Analysis%20(2ed.,%20Springer,%202002)(518s)_MVsa_.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce dimensionality of data\n",
    "pca = PCA()\n",
    "pca_feat =  # CODE HERE\n",
    "pca_feat  # What is this output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Principal Components to retain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output PCA table\n",
    "pd.DataFrame(\n",
    "    {\"Eigenvalue\": pca.explained_variance_,\n",
    "     \"Difference\": np.insert(np.diff(pca.explained_variance_), 0, 0),\n",
    "     \"Proportion\": pca.explained_variance_ratio_,\n",
    "     \"Cumulative\": np.cumsum(pca.explained_variance_ratio_)},\n",
    "    index=range(1, pca.n_components_ + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure and axes\n",
    "# CODE HERE: INITIALIZE A FIGURE AND 2 AXIS (ax1 and ax2)\n",
    "\n",
    "# draw plots\n",
    "ax1.plot(?, marker=\".\", markersize=12)  # CODE HERE: PLOT THE EIGENVALUES (EXPLAINED VARIANCE)\n",
    "ax2.plot(?, marker=\".\", markersize=12, label=\"Proportion\")  # CODE HERE: PLOT THE EXPLAINED VARIANCE RATIO\n",
    "ax2.plot(?, marker=\".\", markersize=12, linestyle=\"--\", label=\"Cumulative\")  # CODE HERE: PLOT THE CUMULATIVE EXPLAINED VARIANCE RATIO\n",
    "\n",
    "# customizations\n",
    "ax2.legend()\n",
    "ax1.set_title(\"Scree Plot\", fontsize=14)\n",
    "ax2.set_title(\"Variance Explained\", fontsize=14)\n",
    "ax1.set_ylabel(\"Eigenvalue\")\n",
    "ax2.set_ylabel(\"Proportion\")\n",
    "ax1.set_xlabel(\"Components\")\n",
    "ax2.set_xlabel(\"Components\")\n",
    "ax1.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax1.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "ax2.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax2.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform PCA again with the number of principal components you want to retain\n",
    "pca = # CODE HERE\n",
    "pca_feat = # CODE HERE\n",
    "pca_feat_names = [f\"PC{i}\" for i in range(pca.n_components_)]\n",
    "pca_df = pd.DataFrame(pca_feat, index=df_pca.index, columns=pca_feat_names)  # remember index=df_pca.index\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain pca variables\n",
    "df_pca = pd.concat([df_pca, pca_df], axis=1)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we interpret each Principal Component (with style)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _color_red_or_green(val):\n",
    "    if val < -0.45:\n",
    "        color = 'background-color: red'\n",
    "    elif val > 0.45:\n",
    "        color = 'background-color: green'\n",
    "    else:\n",
    "        color = ''\n",
    "    return color\n",
    "\n",
    "# Interpreting each Principal Component\n",
    "loadings = # CODE HERE: Obtain the loadings (i.e. correlation between PCs and original features)\n",
    "loadings.style.applymap(_color_red_or_green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pca.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some final data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this after checking the new pandas profiling report\n",
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo data exploration\n",
    "\n",
    "Check if the data looks the way you expect it to. \n",
    "\n",
    "- Have you missed some outliers? \n",
    "- Are there still missing values?\n",
    "- Is the data normalized?\n",
    "\n",
    "This is an iterative process. It is likely you will change your preprocessing steps frequently throughout your group work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(\n",
    "    df,\n",
    "    title='Tugas Customer Data Preprocessed',\n",
    "    correlations={\n",
    "        \"pearson\": {\"calculate\": True},\n",
    "        \"spearman\": {\"calculate\": False},\n",
    "        \"kendall\": {\"calculate\": False},\n",
    "        \"phi_k\": {\"calculate\": False},\n",
    "        \"cramers\": {\"calculate\": False},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is everything as you expect it to be? Save the data for later use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this situation we don't need to save the index of the observations\n",
    "df.to_csv(os.path.join(\"..\", \"data\", \"tugas_preprocessed.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
